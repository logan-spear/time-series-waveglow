{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, train_f=\"wind_power_data/wind_power_train.pickle\", test_f = \"wind_power_data/wind_power_test.pickle\", n=96, rolling=True, small_subset=False, directory='./'):\n",
    "        self.trainset = pd.read_pickle(directory+'/'+train_f).values\n",
    "        self.testset = pd.read_pickle(directory+'/'+test_f).values\n",
    "        self.m = self.trainset.shape[0]\n",
    "        self.m_test = self.testset.shape[0]\n",
    "        self.n = n\n",
    "        self.rolling = rolling\n",
    "        self.small_subset = small_subset\n",
    "        if self.rolling:\n",
    "            if small_subset:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "        else:\n",
    "            self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        self.num_samples = self.sample_indices.shape[0]\n",
    "        self.sample_idx = 0\n",
    "        self.epoch_end = True\n",
    "\n",
    "    def sample(self, batch_size=24):\n",
    "\n",
    "        if self.sample_idx+batch_size >= self.num_samples:\n",
    "            self.epoch_end = False\n",
    "            indices = self.sample_indices[self.sample_idx:]\n",
    "            self.sample_idx = 0\n",
    "            if self.rolling:\n",
    "                if self.small_subset:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "                else:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        else:\n",
    "            indices = self.sample_indices[self.sample_idx:self.sample_idx+batch_size]\n",
    "            self.sample_idx += batch_size\n",
    "\n",
    "        context = np.vstack([np.reshape(self.trainset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        context = context[:, :, None]\n",
    "\n",
    "        forecast = np.vstack([np.reshape(self.trainset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        return context, forecast\n",
    "\n",
    "    def test_samples(self, num_contexts=15):\n",
    "        indices = np.random.choice(list(range(self.n, self.m_test-self.n+1, self.n)), num_contexts, replace=False)\n",
    "        context = np.vstack([np.reshape(self.testset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        context = np.reshape(context, [num_contexts, self.n])\n",
    "        context = context[:, :, None]\n",
    "        forecast = np.reshape(forecast, [num_contexts, self.n])\n",
    "\n",
    "        return context, forecast\n",
    "    \n",
    "    def test_data(self):\n",
    "        context = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.test.shape[0]-self.n)])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.n, self.test.shape[0])])\n",
    "        return context, forecast\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from DataLoader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from waveglow_model import WaveGlow, WaveGlowLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='argument parser')\n",
    "# parser.add_argument('--epochs', dest='epochs', type=int, default=100)\n",
    "# parser.add_argument('--rolling', dest='rolling', type=int, default=1)\n",
    "# parser.add_argument('--small_subset', dest='small_subset', type=int, default=0)\n",
    "# parser.add_argument('--use_gpu', dest='use_gpu', type=int, default=1)\n",
    "# parser.add_argument('--checkpointing', dest='checkpointing', type=int, default=1)\n",
    "# parser.add_argument('--generate_per_epoch', dest='generate_per_epoch', type=int, default=1)\n",
    "# parser.add_argument('--generate_final', dest='generate_final', type=int, default=1)\n",
    "# parser.add_argument('--batch_size', dest='batch_size', type=int, default=12)\n",
    "# parser.add_argument('--learning_rate', dest='learning_rate', type=float, default=1e-4)\n",
    "# parser.add_argument('--n_context_channels', dest='n_context_channels', type=int, default=96)\n",
    "# parser.add_argument('--n_flows', dest='n_flows', type=int, default=6)\n",
    "# parser.add_argument('--n_group', dest='n_group', type=int, default=24)\n",
    "# parser.add_argument('--n_early_every', dest='n_early_every', type=int, default=3)\n",
    "# parser.add_argument('--n_early_size', dest='n_early_size', type=int, default=6)\n",
    "# parser.add_argument('--n_layers', dest='n_layers', type=int, default=4)\n",
    "# parser.add_argument('--dilation_list', dest='dilation_list', type=str, default='1 1 2 2')\n",
    "# parser.add_argument('--n_channels', dest='n_channels', type=int, default=96)\n",
    "# parser.add_argument('--kernel_size', dest='kernel_size', type=int, default=3)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.rolling = True if args.rolling else False\n",
    "# args.small_subset = True if args.small_subset else False\n",
    "# args.use_gpu = True if args.use_gpu else False\n",
    "# args.checkpointing = True if args.checkpointing else False\n",
    "# args.dilation_list = [int(i) for i in args.dilation_list.split(' ')]\n",
    "\n",
    "def mse_loss(context, forecast, model, use_gpu, generations_per_sample=20):\n",
    "    mse_loss = 0.0\n",
    "    for i in range(generations_per_sample):\n",
    "        if use_gpu:\n",
    "            context = torch.cuda.FloatTensor(context)\n",
    "        else:\n",
    "            context = torch.FloatTensor(context)\n",
    "\n",
    "        if use_gpu:\n",
    "            gen_forecast = model.generate(context)\n",
    "        else:\n",
    "            gen_forecast = model.generate(context).cpu()\n",
    "\n",
    "        mse_loss += np.square(gen_forecast-forecast).mean(axis=1)\n",
    "\n",
    "\n",
    "    print(\"Test MSE Loss: %.4f\" % mse_loss)\n",
    "    return mse_loss\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert(os.path.isfile(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    model_for_loading = checkpoint_dict['model']\n",
    "    model.load_state_dict(model_for_loading.state_dict())\n",
    "    print(\"Loaded checkpoint '%s' (iteration %d)\" % (checkpoint_path, iteration))\n",
    "    return model, optimizer, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath, use_gpu=True):\n",
    "    print(\"Saving model and optimizer state at iteration %d to %s\" % (iteration, filepath))\n",
    "\n",
    "\n",
    "    model_for_saving = model\n",
    "\n",
    "    model_for_saving.load_state_dict(model.state_dict())\n",
    "    torch.save({'model': model_for_saving,\n",
    "                'iteration': iteration,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "\n",
    "\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True\n",
    "def training_procedure(dataset=None, num_gpus=0, output_directory='./train', epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=False, mname='model'):\n",
    "    print(\"#############\")\n",
    "    params.append(use_gpu)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     if not os.path.isdir(output_directory[2:]): os.mkdir(output_directory[2:])\n",
    "    if checkpointing and not os.path.isdir(checkpoint_path[2:]): os.mkdir(checkpoint_path[2:])\n",
    "    criterion = WaveGlowLoss()\n",
    "    model = WaveGlow(*params)\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # iteration = 0\n",
    "    # if checkpoint_path != \"\":\n",
    "        # model, optimizer, iteration = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "        # iteration += 1\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    loss_iteration = []\n",
    "    for epoch in range(epochs):\n",
    "        iteration = 0\n",
    "        print(\"Epoch: %d/%d\" % (epoch+1, epochs))\n",
    "        avg_loss = []\n",
    "        while(dataset.epoch_end):\n",
    "            # model.zero_grad()\n",
    "            context, forecast = dataset.sample(batch_size)\n",
    "\n",
    "            if use_gpu:\n",
    "                forecast = torch.autograd.Variable(torch.cuda.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.cuda.FloatTensor(context))\n",
    "            else:\n",
    "                forecast = torch.autograd.Variable(torch.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.FloatTensor(context))\n",
    "\n",
    "            z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "\n",
    "            loss = criterion((z, log_s_list, log_det_w_list))\n",
    "            reduced_loss = loss.item()\n",
    "            loss_iteration.append(reduced_loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            avg_loss.append(reduced_loss)\n",
    "            optimizer.step()\n",
    "            print(\"Model waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d\" % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], learning_rate, seed))\n",
    "            print(\"On iteration %d with loss %.4f\" % (iteration, reduced_loss))\n",
    "            iteration += 1\n",
    "            # if (checkpointing and (iteration % iters_per_checkpoint == 0)):\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True      epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=True):\n",
    "\n",
    "\n",
    "        if gen_tests: generate_tests(dataset, model, 5, 96, use_gpu, str(epoch+1))\n",
    "        epoch_loss = sum(avg_loss)/len(avg_loss)\n",
    "        if checkpointing:\n",
    "            checkpoint_path = \"%s/%s/epoch-%d_loss-%.4f\" % (output_directory, mname, epoch, epoch_loss)\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "\n",
    "        dataset.epoch_end = True\n",
    "        \n",
    "    context, forecast = dataset.test_data()\n",
    "    z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "    \n",
    "    \n",
    "    loss = criterion((z, log_s_list, log_det_w_list))\n",
    "    test_loss = loss.item()    \n",
    "    test_mse = mse_loss(test_context, test_forecast, model, use_gpu)\n",
    "    \n",
    "    checkpoint_path = \"%s/%s/finalmodel_epoch-%d_testloss-%.4f_testmse_%.4f\" % (output_directory, mname, epoch, test_loss, test_mse)\n",
    "    save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "    \n",
    "    print(\"Test loss for this model is %.5f\" % test_loss)\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_iteration)), np.log10(np.array(loss_iteration)+1.0))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('log10 of loss')\n",
    "    plt.savefig('total_loss_graph.png')\n",
    "    plt.close()\n",
    "    return test_loss, model\n",
    "\n",
    "def generate_tests(dataset, model, num_contexts=15, n=96, use_gpu=True, epoch='final', batch_size=24, output_directory='generated', mname='model'):\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    context, forecast = dataset.test_samples(num_contexts=num_contexts)\n",
    "    if use_gpu:\n",
    "        context = torch.cuda.FloatTensor(context)\n",
    "    else:\n",
    "        context = torch.FloatTensor(context)\n",
    "\n",
    "    if use_gpu:\n",
    "        gen_forecast = model.generate(context).cpu()\n",
    "    else:\n",
    "        gen_forecast = model.generate(context)\n",
    "\n",
    "    for i in range(num_contexts, ):\n",
    "        plt.figure()\n",
    "        plt.plot(range(n), gen_forecast[i, :], label='generated')\n",
    "        plt.plot(range(n), forecast[i, :], label='original')\n",
    "        plt.legend()\n",
    "        plt.xlabel('time (t)')\n",
    "        plt.savefig('$s/%s/forecast_generated_%d_epoch-%s.png' % (output_directory, mname, i, epoch))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def training_generator(config, epochs=100, batch_size=24, generate_per_epoch=True, checkpointing=False, use_gpu=False, n_channels=96, n_context_channels=96, rolling=True, dataset=None):\n",
    "    # n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True\n",
    "    # params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3]\n",
    "    \n",
    "    multiple = [1, 2]\n",
    "#     if config[\"multiple\"]==0:\n",
    "#         multiple = [1, 2]\n",
    "#     elif config[\"multiple\"]==1:\n",
    "#         multiple = [2, 4]\n",
    "    \n",
    "\n",
    "    dilation_list = [multiple[0]]*int((config[\"dilation_rate\"]*.25 + .25)*config[\"n_layers\"]) + [multiple[1]]*int((1-(config[\"dilation_rate\"]*.25 + .25))*config[\"n_layers\"])\n",
    "\n",
    "    params = [n_context_channels,\n",
    "                config[\"n_flows\"],\n",
    "                config[\"n_group\"],\n",
    "                config[\"n_early_every\"],\n",
    "                config[\"n_early_size\"],\n",
    "                config[\"n_layers\"],\n",
    "                dilation_list,\n",
    "                n_channels,\n",
    "                config[\"kernel_size\"]]\n",
    "#     dataset = deepcopy(config[\"dataset\"])\n",
    "    dataset = config[\"dataset\"]\n",
    "#     if dataset==None:\n",
    "#         dataset = DataLoader(rolling=rolling, small_subset=False)\n",
    "    output_directory = './train'\n",
    "    \n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    mname = 'waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d' % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], learning_rate, seed)\n",
    "    if not os.path.isdir(output_directory+mname):\n",
    "        os.mkdir(output_directory+'/'+mname)\n",
    "        \n",
    "    test_loss, final_model = training_procedure(epochs=epochs, \n",
    "                            dataset=dataset, \n",
    "                            use_gpu=use_gpu, \n",
    "                            checkpointing=checkpointing, \n",
    "                            gen_tests=generate_per_epoch, \n",
    "                            batch_size=batch_size, \n",
    "                            learning_rate=config[\"learning_rate\"],\n",
    "                            params=params,\n",
    "                            mname=mname,\n",
    "                            output_directory=output_directory)\n",
    "    if generate_final:\n",
    "        generate_tests(dataset, final_model, use_gpu=use_gpu, mname=mname)\n",
    "        \n",
    "    return test_loss\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, os\n",
    "from ray.tune import run\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "    # n_context_channels=96, n_flows=6, n_group=24, \n",
    "    #n_early_every=3, n_early_size=8, \n",
    "    #n_layers=2, dilation_list=[1,2], n_channels=96, \n",
    "    #kernel_size=3, use_gpu=True\n",
    "    # params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3]\n",
    "\n",
    "dataset = DataLoader(rolling=True, small_subset=False)\n",
    "space = {'n_flows': hp.choice('n_flows', np.arange(6, 7, dtype=int)),\n",
    "        'n_group': hp.choice('n_group', np.arange(24, 25, dtype=int)),\n",
    "#         'n_early_every': hp.choice('n_early_every', np.arange(1, 3, dtype=int)),\n",
    "         'n_early_every': hp.choice('n_early_every', np.arange(3, 4, dtype=int)),\n",
    "        'n_early_size': hp.choice('n_early_size', np.arange(2, 8, dtype=int)),\n",
    "        'n_layers': hp.choice('n_layers', np.arange(20, 24, 4, dtype=int)),\n",
    "#         'dilation_rate': hp.choice('dilation_rate', [0, 1, 2, 3]),\n",
    "         'dilation_rate': hp.choice('dilation_rate', [0, 1]),\n",
    "        'multiple': hp.choice('multiple', [0, 1]),\n",
    "        'kernel_size': hp.choice('kernel_size', [1, 3]),\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(1, 20, dtype=int)),\n",
    "        'dataset': dataset}\n",
    "\n",
    "algo = HyperOptSearch(space, \n",
    "                      max_concurrent=1, \n",
    "                      mode=\"min\")\n",
    "\n",
    "# scheduler = AsyncHyperBandScheduler(metric=\"test_loss\", mode=\"min\")\n",
    "analysis = run(training_generator, search_alg=algo, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
