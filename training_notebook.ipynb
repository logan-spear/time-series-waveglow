{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the cells wider in the browser window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from DataLoader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from waveglow_model import WaveGlow, WaveGlowLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train_f=\"wind_power_data/wind_power_train.pickle\", test_f = \"wind_power_data/wind_power_test.pickle\", n=96, rolling=True, small_subset=False, directory='./'):\n",
    "        self.trainset = pd.read_pickle(directory+'/'+train_f).values\n",
    "        self.testset = pd.read_pickle(directory+'/'+test_f).values\n",
    "        self.m = self.trainset.shape[0]\n",
    "        self.m_test = self.testset.shape[0]\n",
    "        self.n = n\n",
    "        self.rolling = rolling\n",
    "        self.small_subset = small_subset\n",
    "        if self.rolling:\n",
    "            if small_subset:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "        else:\n",
    "            self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        self.num_samples = self.sample_indices.shape[0]\n",
    "        self.sample_idx = 0\n",
    "        self.epoch_end = True\n",
    "\n",
    "    def sample(self, batch_size=24):\n",
    "\n",
    "        if self.sample_idx+batch_size >= self.num_samples:\n",
    "            self.epoch_end = False\n",
    "            indices = self.sample_indices[self.sample_idx:]\n",
    "            self.sample_idx = 0\n",
    "            if self.rolling:\n",
    "                if self.small_subset:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "                else:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        else:\n",
    "            indices = self.sample_indices[self.sample_idx:self.sample_idx+batch_size]\n",
    "            self.sample_idx += batch_size\n",
    "\n",
    "        context = np.vstack([np.reshape(self.trainset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        context = context[:, :, None]\n",
    "\n",
    "        forecast = np.vstack([np.reshape(self.trainset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        return context, forecast\n",
    "\n",
    "    def test_samples(self, num_contexts=15):\n",
    "        indices = np.random.choice(list(range(self.n, self.m_test-self.n+1, self.n)), num_contexts, replace=False)\n",
    "        context = np.vstack([np.reshape(self.testset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        context = np.reshape(context, [num_contexts, self.n])\n",
    "        context = context[:, :, None]\n",
    "        forecast = np.reshape(forecast, [num_contexts, self.n])\n",
    "\n",
    "        return context, forecast\n",
    "    \n",
    "    def test_data(self):\n",
    "        context = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.testset.shape[0]-2*self.n)])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.n, self.testset.shape[0]-self.n)])\n",
    "        return context, forecast\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='argument parser')\n",
    "# parser.add_argument('--epochs', dest='epochs', type=int, default=100)\n",
    "# parser.add_argument('--rolling', dest='rolling', type=int, default=1)\n",
    "# parser.add_argument('--small_subset', dest='small_subset', type=int, default=0)\n",
    "# parser.add_argument('--use_gpu', dest='use_gpu', type=int, default=1)\n",
    "# parser.add_argument('--checkpointing', dest='checkpointing', type=int, default=1)\n",
    "# parser.add_argument('--generate_per_epoch', dest='generate_per_epoch', type=int, default=1)\n",
    "# parser.add_argument('--generate_final', dest='generate_final', type=int, default=1)\n",
    "# parser.add_argument('--batch_size', dest='batch_size', type=int, default=12)\n",
    "# parser.add_argument('--learning_rate', dest='learning_rate', type=float, default=1e-4)\n",
    "# parser.add_argument('--n_context_channels', dest='n_context_channels', type=int, default=96)\n",
    "# parser.add_argument('--n_flows', dest='n_flows', type=int, default=6)\n",
    "# parser.add_argument('--n_group', dest='n_group', type=int, default=24)\n",
    "# parser.add_argument('--n_early_every', dest='n_early_every', type=int, default=3)\n",
    "# parser.add_argument('--n_early_size', dest='n_early_size', type=int, default=6)\n",
    "# parser.add_argument('--n_layers', dest='n_layers', type=int, default=4)\n",
    "# parser.add_argument('--dilation_list', dest='dilation_list', type=str, default='1 1 2 2')\n",
    "# parser.add_argument('--n_channels', dest='n_channels', type=int, default=96)\n",
    "# parser.add_argument('--kernel_size', dest='kernel_size', type=int, default=3)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.rolling = True if args.rolling else False\n",
    "# args.small_subset = True if args.small_subset else False\n",
    "# args.use_gpu = True if args.use_gpu else False\n",
    "# args.checkpointing = True if args.checkpointing else False\n",
    "# args.dilation_list = [int(i) for i in args.dilation_list.split(' ')]\n",
    "\n",
    "def mse_loss(context, forecast, model, use_gpu, generations_per_sample=20):\n",
    "    mse_loss = 0.0\n",
    "    for i in range(generations_per_sample):\n",
    "        if use_gpu:\n",
    "            context = torch.cuda.FloatTensor(context)\n",
    "        else:\n",
    "            context = torch.FloatTensor(context)\n",
    "\n",
    "        if use_gpu:\n",
    "            gen_forecast = model.generate(context)\n",
    "        else:\n",
    "            gen_forecast = model.generate(context).cpu()\n",
    "\n",
    "        mse_loss += np.square(gen_forecast-forecast).mean(axis=1)\n",
    "\n",
    "\n",
    "    print(\"Test MSE Loss: %.4f\" % mse_loss)\n",
    "    return mse_loss\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert(os.path.isfile(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    model_for_loading = checkpoint_dict['model']\n",
    "    model.load_state_dict(model_for_loading.state_dict())\n",
    "    print(\"Loaded checkpoint '%s' (iteration %d)\" % (checkpoint_path, iteration))\n",
    "    return model, optimizer, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath, use_gpu=True):\n",
    "    print(\"Saving model and optimizer state at iteration %d to %s\" % (iteration, filepath))\n",
    "\n",
    "\n",
    "    model_for_saving = model\n",
    "\n",
    "    model_for_saving.load_state_dict(model.state_dict())\n",
    "    torch.save({'model': model_for_saving,\n",
    "                'iteration': iteration,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "\n",
    "\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True\n",
    "def training_procedure(dataset=None, num_gpus=0, output_directory='./train', epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=False, mname='model'):\n",
    "    print(\"#############\")\n",
    "    params.append(use_gpu)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     if not os.path.isdir(output_directory[2:]): os.mkdir(output_directory[2:])\n",
    "    if checkpointing and not os.path.isdir(checkpoint_path[2:]): os.mkdir(checkpoint_path[2:])\n",
    "    criterion = WaveGlowLoss()\n",
    "    model = WaveGlow(*params)\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # iteration = 0\n",
    "    # if checkpoint_path != \"\":\n",
    "        # model, optimizer, iteration = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "        # iteration += 1\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    loss_iteration = []\n",
    "    for epoch in range(epochs):\n",
    "        iteration = 0\n",
    "        print(\"Epoch: %d/%d\" % (epoch+1, epochs))\n",
    "        avg_loss = []\n",
    "        while(dataset.epoch_end):\n",
    "            # model.zero_grad()\n",
    "            context, forecast = dataset.sample(batch_size)\n",
    "\n",
    "            if use_gpu:\n",
    "                forecast = torch.autograd.Variable(torch.cuda.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.cuda.FloatTensor(context))\n",
    "            else:\n",
    "                forecast = torch.autograd.Variable(torch.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.FloatTensor(context))\n",
    "\n",
    "            z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "\n",
    "            loss = criterion((z, log_s_list, log_det_w_list))\n",
    "            reduced_loss = loss.item()\n",
    "            loss_iteration.append(reduced_loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            avg_loss.append(reduced_loss)\n",
    "            optimizer.step()\n",
    "#             print(\"Model waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d\" % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], learning_rate, seed))\n",
    "            print(\"On iteration %d with loss %.4f\" % (iteration, reduced_loss))\n",
    "            iteration += 1\n",
    "            # if (checkpointing and (iteration % iters_per_checkpoint == 0)):\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True      epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=True):\n",
    "\n",
    "\n",
    "        if gen_tests: generate_tests(dataset, model, 5, 96, use_gpu, str(epoch+1))\n",
    "        epoch_loss = sum(avg_loss)/len(avg_loss)\n",
    "        if checkpointing:\n",
    "            checkpoint_path = \"%s/%s/epoch-%d_loss-%.4f\" % (output_directory, mname, epoch, epoch_loss)\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "\n",
    "        dataset.epoch_end = True\n",
    "        \n",
    "    context, forecast = dataset.test_data()\n",
    "    context = context[:, :, None]\n",
    "    context = torch.FloatTensor(context)\n",
    "    forecast = torch.FloatTensor(forecast)\n",
    "    z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "    \n",
    "    \n",
    "    loss = criterion((z, log_s_list, log_det_w_list))\n",
    "    test_loss = loss.item()    \n",
    "    test_mse = mse_loss(context, forecast, model, use_gpu)\n",
    "    \n",
    "    checkpoint_path = \"%s/%s/finalmodel_epoch-%d_testloss-%.4f_testmse_%.4f\" % (output_directory, mname, epoch, test_loss, test_mse)\n",
    "    save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "    \n",
    "    print(\"Test loss for this model is %.5f\" % test_loss)\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_iteration)), np.log10(np.array(loss_iteration)+1.0))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('log10 of loss')\n",
    "    plt.savefig('total_loss_graph.png')\n",
    "    plt.close()\n",
    "    return test_loss, model\n",
    "\n",
    "def generate_tests(dataset, model, num_contexts=15, n=96, use_gpu=True, epoch='final', batch_size=24, output_directory='generated', mname='model'):\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    context, forecast = dataset.test_samples(num_contexts=num_contexts)\n",
    "    if use_gpu:\n",
    "        context = torch.cuda.FloatTensor(context)\n",
    "    else:\n",
    "        context = torch.FloatTensor(context)\n",
    "\n",
    "    if use_gpu:\n",
    "        gen_forecast = model.generate(context).cpu()\n",
    "    else:\n",
    "        gen_forecast = model.generate(context)\n",
    "        \n",
    "    if not os.path.isdir(output_directory + \"/\" + mname):\n",
    "        os.mkdir(output_directory+\"/\"+mname)\n",
    "        \n",
    "    print(\"Done generating forecasts using model, now generating plots\")\n",
    "\n",
    "    for i in range(num_contexts):\n",
    "        plt.figure()\n",
    "        plt.plot(range(n), gen_forecast[i, :], label='generated')\n",
    "        plt.plot(range(n), forecast[i, :], label='original')\n",
    "        plt.legend()\n",
    "        plt.xlabel('time (t)')\n",
    "        plt.savefig('%s/%s/forecast_generated_%d_epoch-%s.png' % (output_directory, mname, i, epoch))\n",
    "        plt.close()\n",
    "        print(\"Generated plot %i of %i\" % (i+1, num_contexts))\n",
    "    \n",
    "    print(\"End of generate tests function\")\n",
    "\n",
    "\n",
    "\n",
    "def training_generator(config, epochs=100, batch_size=24, seed=2019, generate_per_epoch=True, generate_final_plots=False, checkpointing=False, use_gpu=False, n_channels=96, n_context_channels=96, rolling=True, dataset=None):\n",
    "    # n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True\n",
    "    # params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3]\n",
    "    print(\"START OF FUNCTION!!!!!!\")\n",
    "    \n",
    "    multiple = [1, 2]\n",
    "#     if config[\"multiple\"]==0:\n",
    "#         multiple = [1, 2]\n",
    "#     elif config[\"multiple\"]==1:\n",
    "#         multiple = [2, 4]\n",
    "    \n",
    "\n",
    "    dilation_list = [multiple[0]]*int((config[\"dilation_rate\"]*.25 + .25)*config[\"n_layers\"]) + [multiple[1]]*int((1-(config[\"dilation_rate\"]*.25 + .25))*config[\"n_layers\"])\n",
    "\n",
    "    params = [n_context_channels,\n",
    "                config[\"n_flows\"],\n",
    "                config[\"n_group\"],\n",
    "                config[\"n_early_every\"],\n",
    "                config[\"n_early_size\"],\n",
    "                config[\"n_layers\"],\n",
    "                dilation_list,\n",
    "                n_channels,\n",
    "                config[\"kernel_size\"]]\n",
    "#     dataset = deepcopy(config[\"dataset\"])\n",
    "    dataset = config[\"dataset\"]\n",
    "#     if dataset==None:\n",
    "#         dataset = DataLoader(rolling=rolling, small_subset=False)\n",
    "    output_directory = './train'\n",
    "    \n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    mname = 'waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d' % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], config[\"learning_rate\"], seed)\n",
    "    if not os.path.isdir(output_directory+\"/\"+mname):\n",
    "        print(\"Making a new diretory at \" + output_directory+\"/\"+mname)\n",
    "        os.mkdir(output_directory+'/'+mname)\n",
    "    else:\n",
    "        print(\"apparently directory already exists: \" + output_directory +\"/\"+mname)\n",
    "        \n",
    "    test_loss, final_model = training_procedure(epochs=epochs, \n",
    "                            dataset=dataset, \n",
    "                            use_gpu=use_gpu, \n",
    "                            checkpointing=checkpointing, \n",
    "                            gen_tests=generate_per_epoch, \n",
    "                            batch_size=batch_size, \n",
    "                            learning_rate=config[\"learning_rate\"],\n",
    "                            seed=seed,\n",
    "                            params=params,\n",
    "                            mname=mname,\n",
    "                            output_directory=output_directory)\n",
    "    \n",
    "    \n",
    "    print(\"Value of generate final plots: \", generate_final_plots)\n",
    "#     if generate_final_plots:\n",
    "#         print(\"Now calling generate_tests\")\n",
    "#         generate_tests(dataset, final_model, use_gpu=use_gpu, mname=mname)\n",
    "    \n",
    "#     print(\"Done with generate_tests function\")\n",
    "        \n",
    "    return test_loss, final_model\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START OF FUNCTION!!!!!!\n",
      "apparently directory already exists: ./train/waveglow_ncontextchannels-96_nflows-6_ngroup-96-nearlyevery-99-nearlysize-8-nlayers-4_dilations-[1, 1, 1, 2]_nchannels_96-kernelsize-3-lr-0.00100_seed-2019\n",
      "#############\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Epoch: 1/1\n",
      "On iteration 0 with loss 41.0296\n",
      "On iteration 1 with loss 20.9502\n",
      "On iteration 2 with loss 9.2058\n",
      "On iteration 3 with loss 4.4824\n",
      "On iteration 4 with loss 2.4999\n",
      "On iteration 5 with loss 2.6587\n",
      "On iteration 6 with loss 2.2412\n",
      "On iteration 7 with loss 2.3925\n",
      "On iteration 8 with loss 2.4320\n",
      "On iteration 9 with loss 2.6220\n",
      "On iteration 10 with loss 2.6279\n",
      "On iteration 11 with loss 2.5087\n",
      "On iteration 12 with loss 2.3797\n",
      "On iteration 13 with loss 2.2947\n",
      "On iteration 14 with loss 2.1735\n",
      "On iteration 15 with loss 2.0315\n",
      "On iteration 16 with loss 1.8647\n",
      "On iteration 17 with loss 1.7684\n",
      "On iteration 18 with loss 1.6431\n",
      "On iteration 19 with loss 1.5067\n",
      "On iteration 20 with loss 1.5728\n",
      "On iteration 21 with loss 1.4169\n",
      "On iteration 22 with loss 1.2157\n",
      "On iteration 23 with loss 1.4640\n",
      "On iteration 24 with loss 1.5250\n",
      "On iteration 25 with loss 1.2552\n",
      "On iteration 26 with loss 1.2319\n",
      "On iteration 27 with loss 1.2469\n",
      "On iteration 28 with loss 1.2141\n",
      "On iteration 29 with loss 1.2753\n",
      "On iteration 30 with loss 1.0974\n",
      "On iteration 31 with loss 1.1837\n",
      "On iteration 32 with loss 1.1289\n",
      "On iteration 33 with loss 1.1094\n",
      "Done generating forecasts using model, now generating plots\n",
      "Generated plot 1 of 5\n",
      "Generated plot 2 of 5\n",
      "Generated plot 3 of 5\n",
      "Generated plot 4 of 5\n",
      "Generated plot 5 of 5\n",
      "End of generate tests function\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d36f2c4c51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           'dataset': dataset}\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f29b369389e5>\u001b[0m in \u001b[0;36mtraining_generator\u001b[0;34m(config, epochs, batch_size, seed, generate_per_epoch, generate_final_plots, checkpointing, use_gpu, n_channels, n_context_channels, rolling, dataset)\u001b[0m\n\u001b[1;32m    242\u001b[0m                             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                             \u001b[0mmname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                             output_directory=output_directory)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f29b369389e5>\u001b[0m in \u001b[0;36mtraining_procedure\u001b[0;34m(dataset, num_gpus, output_directory, epochs, learning_rate, batch_size, checkpointing, checkpoint_path, seed, params, use_gpu, gen_tests, mname)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_s_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_det_w_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_out_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School Work/Coterm/CS 236/time-series-waveglow/waveglow_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, forecast, context)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mlog_det_W_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_det_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mlog_s_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School Work/Coterm/CS 236/time-series-waveglow/waveglow_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, forecast, context, reverse)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mforecast_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_half\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0moutput\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mlog_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_half\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_half\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School Work/Coterm/CS 236/time-series-waveglow/waveglow_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, forecast, context)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mcontext_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             acts = fused_add_tanh_sigmoid_multiply(\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcontext_offset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 n_channels_tensor)\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = DataLoader(train_f=\"wind_power_data/wind_power_development.pickle\", rolling=True, small_subset=False)\n",
    "config = {'n_flows': 6, \n",
    "          'n_group': 96, \n",
    "          'n_early_every': 99, \n",
    "          'n_early_size': 8, \n",
    "          'n_layers': 4, \n",
    "          'dilation_rate': 2, \n",
    "          'multiple': [0, 1], \n",
    "          'kernel_size': 3, \n",
    "          'learning_rate': 0.001, \n",
    "          'dataset': dataset}\n",
    "loss, net = training_generator(config=config, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, os\n",
    "from ray.tune import run\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from hyperopt import hp\n",
    "\n",
    "\n",
    "    # n_context_channels=96, n_flows=6, n_group=24, \n",
    "    #n_early_every=3, n_early_size=8, \n",
    "    #n_layers=2, dilation_list=[1,2], n_channels=96, \n",
    "    #kernel_size=3, use_gpu=True\n",
    "    # params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3]\n",
    "\n",
    "dataset = DataLoader(rolling=True, small_subset=False)\n",
    "space = {'n_flows': hp.choice('n_flows', np.arange(6, 7, dtype=int)),\n",
    "        'n_group': hp.choice('n_group', np.arange(24, 25, dtype=int)),\n",
    "#         'n_early_every': hp.choice('n_early_every', np.arange(1, 3, dtype=int)),\n",
    "         'n_early_every': hp.choice('n_early_every', np.arange(3, 4, dtype=int)),\n",
    "        'n_early_size': hp.choice('n_early_size', np.arange(2, 8, dtype=int)),\n",
    "        'n_layers': hp.choice('n_layers', np.arange(20, 24, 4, dtype=int)),\n",
    "#         'dilation_rate': hp.choice('dilation_rate', [0, 1, 2, 3]),\n",
    "         'dilation_rate': hp.choice('dilation_rate', [0, 1]),\n",
    "        'multiple': hp.choice('multiple', [0, 1]),\n",
    "        'kernel_size': hp.choice('kernel_size', [1, 3]),\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(1, 20, dtype=int)),\n",
    "        'dataset': dataset}\n",
    "\n",
    "algo = HyperOptSearch(space, \n",
    "                      max_concurrent=1, \n",
    "                      mode=\"min\")\n",
    "\n",
    "# scheduler = AsyncHyperBandScheduler(metric=\"test_loss\", mode=\"min\")\n",
    "analysis = run(training_generator, search_alg=algo, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
