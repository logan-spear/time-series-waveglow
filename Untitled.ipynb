{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the cells wider in the browser window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from DataLoader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from waveglow_model import WaveGlow, WaveGlowLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train_f=\"wind_power_data/wind_power_train.pickle\", test_f = \"wind_power_data/wind_power_test.pickle\", n=96, rolling=True, small_subset=False, directory='./'):\n",
    "        self.trainset = pd.read_pickle(directory+'/'+train_f).values\n",
    "        self.testset = pd.read_pickle(directory+'/'+test_f).values\n",
    "        self.m = self.trainset.shape[0]\n",
    "        self.m_test = self.testset.shape[0]\n",
    "        self.n = n\n",
    "        self.rolling = rolling\n",
    "        self.small_subset = small_subset\n",
    "        if self.rolling:\n",
    "            if small_subset:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "        else:\n",
    "            self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        self.num_samples = self.sample_indices.shape[0]\n",
    "        self.sample_idx = 0\n",
    "        self.epoch_end = True\n",
    "\n",
    "    def sample(self, batch_size=24):\n",
    "\n",
    "        if self.sample_idx+batch_size >= self.num_samples:\n",
    "            self.epoch_end = False\n",
    "            indices = self.sample_indices[self.sample_idx:]\n",
    "            self.sample_idx = 0\n",
    "            if self.rolling:\n",
    "                if self.small_subset:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), 2000, replace=False)\n",
    "                else:\n",
    "                    self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1)), self.m-2*self.n, replace=False)\n",
    "            else:\n",
    "                self.sample_indices = np.random.choice(list(range(self.n, self.m-self.n+1, self.n)), int((self.m-self.n)/self.n), replace=False)\n",
    "        else:\n",
    "            indices = self.sample_indices[self.sample_idx:self.sample_idx+batch_size]\n",
    "            self.sample_idx += batch_size\n",
    "\n",
    "        context = np.vstack([np.reshape(self.trainset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        context = context[:, :, None]\n",
    "\n",
    "        forecast = np.vstack([np.reshape(self.trainset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        return context, forecast\n",
    "\n",
    "    def test_samples(self, num_contexts=15):\n",
    "        indices = np.random.choice(list(range(self.n, self.m_test-self.n+1, self.n)), num_contexts, replace=False)\n",
    "        context = np.vstack([np.reshape(self.testset[i-self.n:i], [1, self.n]) for i in indices])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in indices])\n",
    "\n",
    "        context = np.reshape(context, [num_contexts, self.n])\n",
    "        context = context[:, :, None]\n",
    "        forecast = np.reshape(forecast, [num_contexts, self.n])\n",
    "\n",
    "        return context, forecast\n",
    "    \n",
    "    def test_data(self):\n",
    "        context = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.testset.shape[0]-2*self.n)])\n",
    "        forecast = np.vstack([np.reshape(self.testset[i:i+self.n], [1, self.n]) for i in range(self.n, self.testset.shape[0]-self.n)])\n",
    "        return context, forecast\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='argument parser')\n",
    "# parser.add_argument('--epochs', dest='epochs', type=int, default=100)\n",
    "# parser.add_argument('--rolling', dest='rolling', type=int, default=1)\n",
    "# parser.add_argument('--small_subset', dest='small_subset', type=int, default=0)\n",
    "# parser.add_argument('--use_gpu', dest='use_gpu', type=int, default=1)\n",
    "# parser.add_argument('--checkpointing', dest='checkpointing', type=int, default=1)\n",
    "# parser.add_argument('--generate_per_epoch', dest='generate_per_epoch', type=int, default=1)\n",
    "# parser.add_argument('--generate_final', dest='generate_final', type=int, default=1)\n",
    "# parser.add_argument('--batch_size', dest='batch_size', type=int, default=12)\n",
    "# parser.add_argument('--learning_rate', dest='learning_rate', type=float, default=1e-4)\n",
    "# parser.add_argument('--n_context_channels', dest='n_context_channels', type=int, default=96)\n",
    "# parser.add_argument('--n_flows', dest='n_flows', type=int, default=6)\n",
    "# parser.add_argument('--n_group', dest='n_group', type=int, default=24)\n",
    "# parser.add_argument('--n_early_every', dest='n_early_every', type=int, default=3)\n",
    "# parser.add_argument('--n_early_size', dest='n_early_size', type=int, default=6)\n",
    "# parser.add_argument('--n_layers', dest='n_layers', type=int, default=4)\n",
    "# parser.add_argument('--dilation_list', dest='dilation_list', type=str, default='1 1 2 2')\n",
    "# parser.add_argument('--n_channels', dest='n_channels', type=int, default=96)\n",
    "# parser.add_argument('--kernel_size', dest='kernel_size', type=int, default=3)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# args.rolling = True if args.rolling else False\n",
    "# args.small_subset = True if args.small_subset else False\n",
    "# args.use_gpu = True if args.use_gpu else False\n",
    "# args.checkpointing = True if args.checkpointing else False\n",
    "# args.dilation_list = [int(i) for i in args.dilation_list.split(' ')]\n",
    "\n",
    "def mse_loss(context, forecast, model, use_gpu, generations_per_sample=20):\n",
    "    mse_loss = 0.0\n",
    "    for i in range(generations_per_sample):\n",
    "        if use_gpu:\n",
    "            context = torch.cuda.FloatTensor(context)\n",
    "        else:\n",
    "            context = torch.FloatTensor(context)\n",
    "\n",
    "        if use_gpu:\n",
    "            gen_forecast = model.generate(context)\n",
    "        else:\n",
    "            gen_forecast = model.generate(context).cpu()\n",
    "\n",
    "        mse_loss += np.square(gen_forecast-forecast).mean(axis=1)\n",
    "\n",
    "\n",
    "    print(\"Test MSE Loss: %.4f\" % mse_loss)\n",
    "    return mse_loss\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert(os.path.isfile(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    model_for_loading = checkpoint_dict['model']\n",
    "    model.load_state_dict(model_for_loading.state_dict())\n",
    "    print(\"Loaded checkpoint '%s' (iteration %d)\" % (checkpoint_path, iteration))\n",
    "    return model, optimizer, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath, use_gpu=True):\n",
    "    print(\"Saving model and optimizer state at iteration %d to %s\" % (iteration, filepath))\n",
    "\n",
    "\n",
    "    model_for_saving = model\n",
    "\n",
    "    model_for_saving.load_state_dict(model.state_dict())\n",
    "    torch.save({'model': model_for_saving,\n",
    "                'iteration': iteration,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "\n",
    "\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True\n",
    "def training_procedure(dataset=None, num_gpus=0, output_directory='./train', epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=False, mname='model'):\n",
    "    print(\"#############\")\n",
    "    params.append(use_gpu)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "#     if not os.path.isdir(output_directory[2:]): os.mkdir(output_directory[2:])\n",
    "    if checkpointing and not os.path.isdir(checkpoint_path[2:]): os.mkdir(checkpoint_path[2:])\n",
    "    criterion = WaveGlowLoss()\n",
    "    model = WaveGlow(*params)\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # iteration = 0\n",
    "    # if checkpoint_path != \"\":\n",
    "        # model, optimizer, iteration = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "        # iteration += 1\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    loss_iteration = []\n",
    "    for epoch in range(epochs):\n",
    "        iteration = 0\n",
    "        print(\"Epoch: %d/%d\" % (epoch+1, epochs))\n",
    "        avg_loss = []\n",
    "        while(dataset.epoch_end):\n",
    "            # model.zero_grad()\n",
    "            context, forecast = dataset.sample(batch_size)\n",
    "\n",
    "            if use_gpu:\n",
    "                forecast = torch.autograd.Variable(torch.cuda.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.cuda.FloatTensor(context))\n",
    "            else:\n",
    "                forecast = torch.autograd.Variable(torch.FloatTensor(forecast))\n",
    "                context = torch.autograd.Variable(torch.FloatTensor(context))\n",
    "\n",
    "            z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "\n",
    "            loss = criterion((z, log_s_list, log_det_w_list))\n",
    "            reduced_loss = loss.item()\n",
    "            loss_iteration.append(reduced_loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            avg_loss.append(reduced_loss)\n",
    "            optimizer.step()\n",
    "#             print(\"Model waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d\" % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], learning_rate, seed))\n",
    "\n",
    "            print(\"On iteration %d with loss %.4f\" % (iteration, reduced_loss))\n",
    "            iteration += 1\n",
    "            # if (checkpointing and (iteration % iters_per_checkpoint == 0)):\n",
    "# n_context_channels=96, n_flows=6, n_group=24, n_early_every=3, n_early_size=8, n_layers=2, dilation_list=[1,2], n_channels=96, kernel_size=3, use_gpu=True      epochs=1000, learning_rate=1e-4, batch_size=12, checkpointing=True, checkpoint_path=\"./checkpoints\", seed=2019, params = [96, 6, 24, 3, 8, 2, [1,2], 96, 3], use_gpu=True, gen_tests=True):\n",
    "\n",
    "\n",
    "        if gen_tests: generate_tests(dataset, model, 5, 96, use_gpu, str(epoch+1))\n",
    "        epoch_loss = sum(avg_loss)/len(avg_loss)\n",
    "        if checkpointing:\n",
    "            checkpoint_path = \"%s/%s/epoch-%d_loss-%.4f\" % (output_directory, mname, epoch, epoch_loss)\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "\n",
    "        dataset.epoch_end = True\n",
    "        \n",
    "    context, forecast = dataset.test_data()\n",
    "    context = context[:, :, None]\n",
    "    context = torch.FloatTensor(context)\n",
    "    forecast = torch.FloatTensor(forecast)\n",
    "    z, log_s_list, log_det_w_list, early_out_shapes = model(forecast, context)\n",
    "    \n",
    "    \n",
    "    loss = criterion((z, log_s_list, log_det_w_list))\n",
    "    test_loss = loss.item()    \n",
    "    test_mse = mse_loss(context, forecast, model, use_gpu)\n",
    "    \n",
    "    checkpoint_path = \"%s/%s/finalmodel_epoch-%d_testloss-%.4f_testmse_%.4f\" % (output_directory, mname, epoch, test_loss, test_mse)\n",
    "    save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path, use_gpu)\n",
    "    \n",
    "    print(\"Test loss for this model is %.5f\" % test_loss)\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_iteration)), np.log10(np.array(loss_iteration)+1.0))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('log10 of loss')\n",
    "    plt.savefig('total_loss_graph.png')\n",
    "    plt.close()\n",
    "    return test_loss, model\n",
    "\n",
    "def generate_tests(dataset, model, num_contexts=15, n=96, use_gpu=True, epoch='final', batch_size=24, output_directory='generated', mname='model'):\n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    context, forecast = dataset.test_samples(num_contexts=num_contexts)\n",
    "    if use_gpu:\n",
    "        context = torch.cuda.FloatTensor(context)\n",
    "    else:\n",
    "        context = torch.FloatTensor(context)\n",
    "\n",
    "    if use_gpu:\n",
    "        gen_forecast = model.generate(context).cpu()\n",
    "    else:\n",
    "        gen_forecast = model.generate(context)\n",
    "        \n",
    "    if not os.path.isdir(output_directory + \"/\" + mname):\n",
    "        os.mkdir(output_directory+\"/\"+mname)\n",
    "        \n",
    "    print(\"Done generating forecasts using model, now generating plots\")\n",
    "\n",
    "    for i in range(num_contexts):\n",
    "        plt.figure()\n",
    "        plt.plot(range(n), gen_forecast[i, :], label='generated')\n",
    "        plt.plot(range(n), forecast[i, :], label='original')\n",
    "        plt.legend()\n",
    "        plt.xlabel('time (t)')\n",
    "        plt.savefig('%s/%s/forecast_generated_%d_epoch-%s.png' % (output_directory, mname, i, epoch))\n",
    "        plt.close()\n",
    "        print(\"Generated plot %i of %i\" % (i+1, num_contexts))\n",
    "    \n",
    "    print(\"End of generate tests function\")\n",
    "\n",
    "\n",
    "\n",
    "def training_generator(config, epochs=100, batch_size=24, seed=2019, generate_per_epoch=True, generate_final_plots=False, checkpointing=False, use_gpu=False, n_channels=96, n_context_channels=96, rolling=True, dataset=None):\n",
    "    print(\"START OF FUNCTION!!!!!!\")\n",
    "    print(\"asdasdasdasdasd\")\n",
    "    multiple = [1, 2]\n",
    "    dilation_list = [multiple[0]]*int((config[\"dilation_rate\"]*.25 + .25)*config[\"n_layers\"]) + [multiple[1]]*int((1-(config[\"dilation_rate\"]*.25 + .25))*config[\"n_layers\"])\n",
    "\n",
    "    params = [n_context_channels,\n",
    "                config[\"n_flows\"],\n",
    "                config[\"n_group\"],\n",
    "                config[\"n_early_every\"],\n",
    "                config[\"n_early_size\"],\n",
    "                config[\"n_layers\"],\n",
    "                dilation_list,\n",
    "                n_channels,\n",
    "                config[\"kernel_size\"]]\n",
    "#     dataset = deepcopy(config[\"dataset\"])\n",
    "    dataset = config[\"dataset\"]\n",
    "#     if dataset==None:\n",
    "#         dataset = DataLoader(rolling=rolling, small_subset=False)\n",
    "    output_directory = './train'\n",
    "    \n",
    "    if not os.path.isdir(output_directory):\n",
    "        os.mkdir(output_directory)\n",
    "        \n",
    "    mname = 'waveglow_ncontextchannels-%d_nflows-%d_ngroup-%d-nearlyevery-%d-nearlysize-%d-nlayers-%d_dilations-%s_nchannels_%d-kernelsize-%d-lr-%.5f_seed-%d' % (params[0], params[1], params[2], params[3], params[4], params[5], str(params[6]), params[7], params[8], config[\"learning_rate\"], seed)\n",
    "    if not os.path.isdir(output_directory+\"/\"+mname):\n",
    "        print(\"Making a new diretory at \" + output_directory+\"/\"+mname)\n",
    "        os.mkdir(output_directory+'/'+mname)\n",
    "    else:\n",
    "        print(\"apparently directory already exists: \" + output_directory +\"/\"+mname)\n",
    "        \n",
    "    test_loss, final_model = training_procedure(epochs=epochs, \n",
    "                            dataset=dataset, \n",
    "                            use_gpu=use_gpu, \n",
    "                            checkpointing=checkpointing, \n",
    "                            gen_tests=generate_per_epoch, \n",
    "                            batch_size=batch_size, \n",
    "                            learning_rate=config[\"learning_rate\"],\n",
    "                            seed=seed,\n",
    "                            params=params,\n",
    "                            mname=mname,\n",
    "                            output_directory=output_directory)\n",
    "    \n",
    "    \n",
    "    print(\"Value of generate final plots: \", generate_final_plots)\n",
    "#     if generate_final_plots:\n",
    "#         print(\"Now calling generate_tests\")\n",
    "#         generate_tests(dataset, final_model, use_gpu=use_gpu, mname=mname)\n",
    "    \n",
    "#     print(\"Done with generate_tests function\")\n",
    "        \n",
    "    return test_loss, final_model\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START OF FUNCTION!!!!!!\n",
      "apparently directory already exists: ./train/waveglow_ncontextchannels-96_nflows-6_ngroup-96-nearlyevery-99-nearlysize-8-nlayers-4_dilations-[1, 1, 1, 2]_nchannels_96-kernelsize-3-lr-0.00100_seed-2019\n",
      "#############\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Channels:  96\n",
      "Epoch: 1/1\n",
      "On iteration 0 with loss 38.0325\n",
      "On iteration 1 with loss 14.6626\n",
      "On iteration 2 with loss 11.4931\n",
      "On iteration 3 with loss 4.5100\n",
      "On iteration 4 with loss 2.6129\n",
      "On iteration 5 with loss 2.2923\n",
      "On iteration 6 with loss 2.1570\n",
      "On iteration 7 with loss 2.3594\n",
      "On iteration 8 with loss 2.4815\n",
      "On iteration 9 with loss 2.5842\n",
      "On iteration 10 with loss 2.6107\n",
      "On iteration 11 with loss 2.6363\n",
      "On iteration 12 with loss 2.4638\n",
      "On iteration 13 with loss 2.1598\n",
      "On iteration 14 with loss 2.2402\n",
      "On iteration 15 with loss 1.9563\n",
      "On iteration 16 with loss 1.8443\n",
      "On iteration 17 with loss 1.7596\n",
      "On iteration 18 with loss 1.6546\n",
      "On iteration 19 with loss 1.6141\n",
      "On iteration 20 with loss 1.4966\n",
      "On iteration 21 with loss 1.3674\n",
      "On iteration 22 with loss 1.3814\n",
      "On iteration 23 with loss 1.2997\n",
      "On iteration 24 with loss 1.3716\n",
      "On iteration 25 with loss 1.1741\n",
      "On iteration 26 with loss 1.0452\n",
      "On iteration 27 with loss 1.2122\n",
      "On iteration 28 with loss 1.1862\n",
      "On iteration 29 with loss 1.2206\n",
      "On iteration 30 with loss 1.1278\n",
      "On iteration 31 with loss 1.0698\n",
      "On iteration 32 with loss 1.0636\n",
      "On iteration 33 with loss 1.1112\n",
      "Done generating forecasts using model, now generating plots\n",
      "Generated plot 1 of 5\n",
      "Generated plot 2 of 5\n",
      "Generated plot 3 of 5\n",
      "Generated plot 4 of 5\n",
      "Generated plot 5 of 5\n",
      "End of generate tests function\n"
     ]
    }
   ],
   "source": [
    "dataset = DataLoader(train_f=\"wind_power_data/wind_power_development.pickle\", rolling=True, small_subset=False)\n",
    "config = {'n_flows': 6, \n",
    "          'n_group': 96, \n",
    "          'n_early_every': 99, \n",
    "          'n_early_size': 8, \n",
    "          'n_layers': 4, \n",
    "          'dilation_rate': 2, \n",
    "          'multiple': [0, 1], \n",
    "          'kernel_size': 3, \n",
    "          'learning_rate': 0.001, \n",
    "          'dataset': dataset}\n",
    "loss, net = training_generator(config=config, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
